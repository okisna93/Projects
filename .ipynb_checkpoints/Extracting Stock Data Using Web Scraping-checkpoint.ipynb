{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee575e5a",
   "metadata": {},
   "source": [
    "## Extracting Stock Data Using Web Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "908dc6ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: mamba in c:\\users\\okanc\\anaconda3\\lib\\site-packages (0.11.2)\n",
      "Requirement already satisfied: coverage in c:\\users\\okanc\\anaconda3\\lib\\site-packages (from mamba) (7.2.2)\n",
      "Requirement already satisfied: clint in c:\\users\\okanc\\anaconda3\\lib\\site-packages (from mamba) (0.5.1)\n",
      "Requirement already satisfied: args in c:\\users\\okanc\\anaconda3\\lib\\site-packages (from clint->mamba) (0.1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: mamba [-h] [--version] [--slow SLOW] [--enable-coverage]\n",
      "             [--coverage-file COVERAGE_FILE] [--format FORMAT] [--no-color]\n",
      "             [--tags TAGS]\n",
      "             [specs ...]\n",
      "mamba: error: unrecognized arguments: -y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: lxml==4.6.4 in c:\\users\\okanc\\anaconda3\\lib\\site-packages (4.6.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: mamba [-h] [--version] [--slow SLOW] [--enable-coverage]\n",
      "             [--coverage-file COVERAGE_FILE] [--format FORMAT] [--no-color]\n",
      "             [--tags TAGS]\n",
      "             [specs ...]\n",
      "mamba: error: unrecognized arguments: -y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: plotly==5.3.1 in c:\\users\\okanc\\anaconda3\\lib\\site-packages (5.3.1)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in c:\\users\\okanc\\anaconda3\\lib\\site-packages (from plotly==5.3.1) (8.0.1)\n",
      "Requirement already satisfied: six in c:\\users\\okanc\\anaconda3\\lib\\site-packages (from plotly==5.3.1) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install mamba\n",
    "!mamba install bs4==4.10.0 -y\n",
    "!pip install lxml==4.6.4\n",
    "!mamba install html5lib==1.1 -y\n",
    "!pip install plotly==5.3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1319c9a",
   "metadata": {},
   "source": [
    "Not all stock data os abailable via API. We will use beatiful soup to extract historical stock share from web-pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "81bf699d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bbedad",
   "metadata": {},
   "source": [
    "Downloading web page using 'request' library.\n",
    "[https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-PY0220EN-SkillsNetwork/labs/project/netflix_data_webpage.html](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-PY0220EN-SkillsNetwork/labs/project/netflix_data_webpage.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9bfa06b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-PY0220EN-SkillsNetwork/labs/project/netflix_data_webpage.html](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-PY0220EN-SkillsNetwork/labs/project/netflix_data_webpage.html\"\n",
    "data=requests.get(url).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "220b55c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\okanc\\anaconda3\\lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "soup=BeautifulSoup(data,\"html5lib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2113461d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<!--?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"yes\"?--><html><head></head><body><error><code>NoSuchKey</code><message>The specified key does not exist.</message><resource>/cf-courses-data/IBMDeveloperSkillsNetwork-PY0220EN-SkillsNetwork/labs/project/netflix_data_webpage.html](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-PY0220EN-SkillsNetwork/labs/project/netflix_data_webpage.html</resource><requestid>11dc61a0-4190-4843-9760-0116b3bb146f</requestid><httpstatuscode>404</httpstatuscode></error></body></html>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fbe033d5",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [27], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Turning html table into a pandas dataframe\u001b[39;00m\n\u001b[0;32m      2\u001b[0m netflix_data\u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mDataFrame(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mData\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOpen\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHigh\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLow\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClose\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVolume\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m \u001b[43msoup\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtbody\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_all\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtr\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m      6\u001b[0m     col\u001b[38;5;241m=\u001b[39mrow\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtd\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m     date \u001b[38;5;241m=\u001b[39m col[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "# Turning html table into a pandas dataframe\n",
    "netflix_data=pd.DataFrame(columns=['Data','Open','High','Low','Close','Volume'])\n",
    "\n",
    "\n",
    "for row in soup.find(\"tbody\").find_all('tr'):\n",
    "    col=row.find_all(\"td\")\n",
    "    date = col[0].text\n",
    "    Open = col[1].text\n",
    "    high = col[2].text\n",
    "    low = col[3].text\n",
    "    close = col[4].text\n",
    "    adj_close = col[5].text\n",
    "    volume = col[6].text\n",
    "    \n",
    "    netflix_data = netflix_data.append({\"Date\":date, \"Open\":Open, \"High\":high, \"Low\":low, \"Close\":close, \"Adj Close\":adj_close, \"Volume\":volume}, ignore_index=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab804bf6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
